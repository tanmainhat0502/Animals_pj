training:
  num_epochs: 10
  batch_size: 32
  learning_rate: 0.001
  optimizer: "adam"           # Có thể là "adam", "sgd", "rmsprop"
  weight_decay: 0.0001        # Regularization
  momentum: 0.9               # Dùng cho SGD
  lr_scheduler: "step"        # "step", "cosine", hoặc "none"
  step_size: 3                # Dùng cho StepLR
  gamma: 0.1                  # Dùng cho StepLR
  backbone: "resnet50"        # Tên backbone: resnet50, efficientnet_b0, v.v.